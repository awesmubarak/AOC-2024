{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "with open(\"1-1.txt\") as f:\n",
    "    puzzle_input = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "\n",
    "heap_A, heap_B = [], []\n",
    "for line in puzzle_input:\n",
    "    A, B = line.split(\"   \")\n",
    "    heapq.heappush(heap_A, int(A))\n",
    "    heapq.heappush(heap_B, int(B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "count_A = Counter(heap_A)\n",
    "count_B = Counter(heap_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_sim = 0\n",
    "while heap_A:\n",
    "    A = heapq.heappop(heap_A)\n",
    "    total_sim += A * count_B[A]\n",
    "while heap_B:\n",
    "    B = heapq.heappop(heap_B)\n",
    "    total_sim += B * count_A[B]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(total_sim//2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I honestly don't think heaps are useful here at all? Feels like the information is already contained in the Counters. Let's try and just use those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniq_vals = set(count_A.keys()) & set(count_B.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_sim = 0\n",
    "for val in uniq_vals:\n",
    "    total_sim += val * count_A[val] * count_B[val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(total_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In theory this should be faster. We build our frequency table once (O(log n)) and do Counter lookups for each element (O(1) each, O(n) total). The heaps approach is heavier for building and maintaining (O(n log n)), and the same for lookups (O(1)).\n",
    "\n",
    "\n",
    "In practice, this is the timeit result of the earlier heaps approach:\n",
    "\n",
    "    31.2 ns ± 5.19 ns per loop (mean ± std. dev. of 7 runs, 10,000,000 loops each)\n",
    "\n",
    "And this is the result of the current Counters approach:\n",
    "\n",
    "    10.1 μs ± 589 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n",
    "\n",
    "Possible explanations:\n",
    "\n",
    "- Heap is a contiguous array in memory, CPU is optimising very well. Counter lookups are harder for the CPU to optimise as each table lookup is done _after_ the last operation, so we're jumping around memory constantly. \n",
    "- The Heap-approach operations are easier to streamline: multiply-accumulates vs string manipulation and hash table lookups\n",
    "\n",
    "Limitations:\n",
    "\n",
    "- Not currently timing datastructure creation times\n",
    "- Explanations are provided without delving into the assembly code\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
